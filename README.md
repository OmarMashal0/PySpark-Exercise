# ğŸš€ PySpark Exercises & Tasks

A collection of **hands-on PySpark exercises with solutions** to practice **big data processing** using Apache Spark.  
Perfect for **students, data engineers, and interview prep**.

---

## ğŸ“Œ Whatâ€™s Inside
- **RDD Operations** â†’ transformations, actions, grouping, aggregations.  
- **DataFrames & SQL** â†’ filtering, grouping, distinct counts, average salary.  
- **Joins & Preprocessing** â†’ handle missing values, replace nulls, clean data.  
- **Text Processing** â†’ word count, stopwords removal, top frequent words.  
- **File Handling** â†’ read/write CSV & JSON.  

Datasets included:  
- `NullData.csv` â†’ sample sales data with nulls.  
- `russia.txt` â†’ text file for word count.  

---

## ğŸ³ Running with Docker
1. **Clone the repo**  
   ```bash
   git clone https://github.com/OmarMashal0/PySpark-Exercise.git
   cd PySpark-Exercise
   ```
2. **Start the cluster**  
   ```bash
   docker compose -f depi.yaml up -d
   ```
   Containers: Spark master, Spark worker, HDFS (namenode + datanode), Jupyter Notebook.  
3. **Access services**  
   - ğŸ““ Jupyter â†’ [http://localhost:8888](http://localhost:8888)  
   - ğŸ“Š Spark UI â†’ [http://localhost:4040](http://localhost:4040)  
4. **Stop cluster**  
   ```bash
   docker compose -f depi.yaml down
   ```

---

## ğŸ“˜ How to Use
- Open exercises in **Jupyter Notebook** (`Task.ipynb`) and solve them.  
- Compare with provided **solutions**.  
- Run standalone `.py` scripts with Spark Submit:  
  ```bash
  docker exec -it pyspark-container spark-submit scripts/example.py
  ```

---

## ğŸ“Š Example Outputs
**Average Salary**  
```
6000.0
```

**Filter Employees Older than 28**  
```
+---+------+---+------+
| id|  name|age|salary|
+---+------+---+------+
|  2|Mariam| 30|  6000|
|  3|  Omar| 35|  7000|
+---+------+---+------+
```

---

## ğŸ¤ Contributing
Contributions welcome ğŸ‰  
1. Fork â†’ Branch â†’ Commit â†’ PR.  
